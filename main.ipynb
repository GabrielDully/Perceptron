{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Binary Classification of Iris Flowers\n",
    "\n",
    "1. Dataset: Use the Iris dataset, a popular dataset in machine learning. It contains four features: sepal length, sepal width, petal length, and petal width, along with the species of iris (Setosa, Versicolor, or Virginica).\n",
    "\n",
    "2. Objective: We will simplify the problem to binary classification by considering only two classes: Setosa and Versicolor. You will train the perceptron to distinguish between these two classes based on the provided features.\n",
    "\n",
    "3. Steps:\n",
    "\n",
    "    a)Load the Iris dataset.\n",
    "\n",
    "    b) Preprocess the data: Since we're considering only two classes, Setosa and Versicolor, you can select only the corresponding rows from the dataset and use only the first two features (sepal length and sepal width) for simplicity.\n",
    "    \n",
    "    c) Implement the perceptron algorithm to learn a decision boundary that separates the two classes.\n",
    "    Train the perceptron on a portion of the dataset.\n",
    "    \n",
    "    d)Test the perceptron on the remaining portion of the dataset and evaluate its performance (e.g., accuracy).\n",
    "    \n",
    "    e) Evaluation: You can evaluate the performance of your perceptron algorithm by calculating the accuracy, which is the proportion of correctly classified instances over the total number of instances.\n",
    "\n",
    "4. Extension: Once you have a basic perceptron working, you can experiment with different aspects such as learning rate, number of epochs, feature scaling, or even extend it to handle multiple classes using techniques like one-vs-all or one-vs-one.\n",
    "\n",
    "5. This exercise will allow you to test your perceptron algorithm in a simple binary classification task and assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $1^{\\underline{st}}$ stage: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (80, 2)\n",
      "X_test shape: (20, 2)\n",
      "y_train shape: (80,)\n",
      "y_test shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data(dataset_path, column_names):\n",
    "\n",
    "    # Load the dataset using Pandas.\n",
    "    data = pd.read_csv(dataset_path, names=column_names)\n",
    "\n",
    "    return data\n",
    "\n",
    "def map_labels_to_int(data, class_column_name, label_0, label_1):\n",
    "\n",
    "    # Map class labels to integers (e. g., 'category1' -> 0, 'category2' -> 1).\n",
    "    class_mapping = {label_0: 0, label_1: 1}\n",
    "    data[class_column_name] = data[class_column_name].map(class_mapping)\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_features_and_labels(data, class_column_name, features):\n",
    "    \n",
    "    # Extract features and labels.\n",
    "    X = data[features].values\n",
    "    y = data[class_column_name].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def normalize_features(X):\n",
    "    \n",
    "    # Normalize_features.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "def split_test_train_sets(X_scaled, y, test_cardinality, randomness_status):\n",
    "\n",
    "    # Split the dataset into training and testing sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_cardinality, random_state=randomness_status)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def preprocess_data(dataset_path, column_names, class_column_name, label_0, label_1, features, test_cardinality, randomness_status):\n",
    "    \n",
    "    data = load_data(dataset_path, column_names)\n",
    "\n",
    "    data = map_labels_to_int(data, class_column_name, label_0, label_1)\n",
    "\n",
    "    X, y = extract_features_and_labels(data, class_column_name, features)\n",
    "\n",
    "    X_scaled = normalize_features(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_cardinality, random_state=randomness_status)\n",
    "\n",
    "    # Print the shapes of training and testing sets\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "dataset_path = 'iris/iris.data'\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "class_column_name = 'class'\n",
    "label_0 = 'Iris-versicolor'\n",
    "label_1 = 'Iris-setosa'\n",
    "features = ['sepal_length', 'sepal_width']\n",
    "\n",
    "test_cardinality = 0.2\n",
    "randomness_status = 42\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(dataset_path, column_names, class_column_name, label_0, label_1, features, test_cardinality, randomness_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2^{\\underline{nd}}$ stage: Training\n",
    "We are going to define that:\n",
    " - if the dot product $(weight*x) >= 0$, then $y$ should be versicolor (0)\n",
    " - if the dot product $(weight*x) < 0$, then $y$ should be setosa (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(X_train, y_train, number_of_epochs=100, learning_rate=0.1):\n",
    "    \n",
    "    # Variable weight_vector initialization with nÂº input features plus 1 element for threshold.\n",
    "    number_of_features = X_train.shape[1] + 1\n",
    "    weight_vector = np.zeros(number_of_features) + 1\n",
    "\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        for x, y in zip(X_train, y_train):\n",
    "\n",
    "            # Insertion of an element [1] in x in order to enable its inner product with weight_vector.\n",
    "            x_with_threshold = np.concatenate(([1], x))\n",
    "            dot_product = np.dot(weight_vector, x_with_threshold)\n",
    "\n",
    "            # Update of weight_vector in case of misclassification for observation equals 0\n",
    "            if y == 0 and dot_product < 0:\n",
    "                weight_vector = weight_vector + y*x_with_threshold\n",
    "\n",
    "            # Update of weight_vector in case of misclassification for observation equals 1\n",
    "            if y == 1 and dot_product >= 0:                \n",
    "                weight_vector = weight_vector - learning_rate*x_with_threshold\n",
    "        \n",
    "    return weight_vector\n",
    "    \n",
    "trained_weight_vector = train_data(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $3^{\\underline{rd}}$ stage: Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def generate_hypothesis_image(X_test, trained_weight_vector):\n",
    "\n",
    "    # Addition of threshold column to X_test\n",
    "    X_test_with_threshold = np.column_stack((np.ones(len(X_test)), X_test))\n",
    "\n",
    "    # Calculation of Inner products and putting them inside a set called inner_products_set\n",
    "    inner_products_set = np.dot(X_test_with_threshold, trained_weight_vector)\n",
    "\n",
    "    # Creation of set with labels according to hypothesis function (trained_weight_vector)\n",
    "    hypothesis_image = np.where(inner_products_set >= 0, 0, 1)\n",
    "\n",
    "    return hypothesis_image\n",
    "\n",
    "def generate_accuracy(hypothesis_image, y_test):\n",
    "\n",
    "    # Calculation of accuracy verifying percentage of correspondence between hypothesis_image and actual image (y_test).\n",
    "    accuracy = np.mean(hypothesis_image == y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def validate_data(X_test, y_test, trained_weight_vector):\n",
    "\n",
    "    hypothesis_image = generate_hypothesis_image(X_test, trained_weight_vector)\n",
    "\n",
    "    accuracy = generate_accuracy(hypothesis_image, y_test)\n",
    "    print(\"Perceptron accuracy:\", accuracy)\n",
    "\n",
    "    return hypothesis_image, accuracy\n",
    "\n",
    "\n",
    "hypothesis_image, accuracy = validate_data(X_test, y_test, trained_weight_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $4^{\\underline{th}}$ stage: Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_weight_vector(weight_vector):\n",
    "\n",
    "    # Saves the model (trained weight vector) into the the file 'perceptron_weights.npy' inside the directory 'model'\n",
    "    np.save('model/perceptron_weights.npy', weight_vector)\n",
    "\n",
    "storage_weight_vector(trained_weight_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $5^{\\underline{th}}$ stage: Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    I must create a script to predict new data using the weight vector found earlier. I don't know if I am leaving inside this file or if I should create another one: .ipynb or .py?\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    I must create a script to predict new data using the weight vector found earlier. I don't know if I am leaving inside this file or if I should create another one: .ipynb or .py?\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
